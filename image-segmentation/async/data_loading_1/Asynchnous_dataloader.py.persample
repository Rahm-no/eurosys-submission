import subprocess
import threading
import time
import torch
import os
import numpy as np
from queue import Full, Empty
import torch.multiprocessing as mp
from torch.utils.data import DataLoader
import psutil
from runtime.distributed_utils import get_rank, get_world_size
import csv

# Global stop event for monitoring
data_stop_event = mp.Event()

data_resume_event = mp.Event()

class DataProducer(mp.Process):
    def __init__(self, queue, dataset, batch_size, shuffle, pin_memory, device, queue_size, indices, sampler, num_workers):
        super().__init__()
        self.queue = queue
        self.dataset = dataset
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.pin_memory = pin_memory
        self.device = device
        self.indices = indices
        self.sampler = sampler
        self.queue_size = queue_size
        self.num_workers = num_workers
        print("Num_workers", self.num_workers)


        self.rank = get_rank()  # Get the GPU rank for this producer
    def run(self):
        print(f"[GPU Rank {self.rank} | Producer {self.pid}] Starting.")
        while not data_stop_event.is_set():
            # Wait until the producer should resume
            if self.queue.qsize() < self.queue_size - 1:  # There's space in the queue
                data_resume_event.set()  # Resume the producer
            else:
                data_resume_event.clear()  # Pause the producer if the queue is full

            batch = []
            num_processed = 0

            for idx in self.indices:
                if data_stop_event.is_set():
                    break

                print(f"[GPU Rank {self.rank} | Producer {self.pid}] Processing index {idx}")
                sample = self.dataset[idx]
                if sample is None:
                    print(f"[GPU Rank {self.rank} | Producer {self.pid}] Sample is None for index {idx}.")
                    continue
                else:
                    self.queue.put(sample)
            #     batch.append(sample)
            #     num_processed += 1

            #     # When batch reaches the desired size, add it to the queue
            #     if len(batch) == self.batch_size:
            #         self.put_batch(batch)
            #         batch = []

            # # If any leftover batch, add to the queue
            # if batch and not data_stop_event.is_set():
            #     self.put_batch(batch)

            print(f"[GPU Rank {self.rank} | Producer {self.pid}] Finished epoch with {num_processed} samples.")
            data_resume_event.clear()  # Pause the producer until more space is available in the queue



    def put_batch(self, batch):
        try:
            images, labels = zip(*batch)
            images = [torch.from_numpy(image) for image in images]
            batch_images_tensor = torch.stack(images)

            if isinstance(labels[0], np.ndarray):
                labels = [torch.from_numpy(label) for label in labels]
            batch_labels_tensor = torch.stack(labels)

            # Attempt to add the batch to the queue without sleep
            if self.queue.qsize() < 40:  # Ensure it doesn't exceed 11
                try:
                    # Try to put data in the queue
                    self.queue.put((batch_images_tensor, batch_labels_tensor), timeout=1)
                except Full:
                    print(f"Queue is full. Current size: {self.queue.qsize()}. Not adding data.")
            else:
                print(f"Queue reached maximum size ({self.queue.qsize()}). Not adding batch.")
        
        except Exception as e:
            print(f"Error in producer: {e}")

    def _stop(self):
        data_stop_event.set()
       



class AsynchronousLoader(DataLoader):
    def __init__(self, dataset, device, shards, rank, slow_processed_queue, batch_size=1, shuffle=False, pin_memory=True, num_workers=1, queue_size=50, drop_last=True, sampler=None):
        super().__init__(dataset=dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory, num_workers=0, drop_last=drop_last, sampler=sampler)
        self.queue_size = queue_size
      
        self.device = device
        self.shards = shards
        self.num_workers = num_workers
        self.world_size = get_world_size()
        self.rank = rank
        self.slow_processed_queue = slow_processed_queue

        self.epoch_batches = len(self.dataset) // (self.batch_size * self.world_size)  # Total batches per epoch
        
        self.shuffle = shuffle

        self.indices = list(self.sampler) # it gives the indices per gpu
        print(f"GPU Rank {self.rank} is assigned {len(self.indices)} samples.")
        self.queue = mp.Queue(maxsize=self.queue_size)
        
        print(f"Created queue for GPU Rank {self.rank}.")

        self.start_threads()

    def start_threads(self):
        self.producers = []
        total_samples = len(self.indices)
        indices_per_producer = max(1, total_samples // self.num_workers)  # Ensure at least one index per producer
        start_idx = 0

        for i in range(self.num_workers):
            end_idx = min(start_idx + indices_per_producer, total_samples)
            producer_indices = self.indices[start_idx:end_idx]
            start_idx = end_idx

            if not producer_indices:  # Break if no indices are available
                break

            print(f"[GPU Rank {self.rank}] Producer {i} assigned indices: {producer_indices}")
            
            producer = DataProducer(self.queue, self.dataset, self.batch_size, self.shuffle,
                                    self.pin_memory, self.device, self.queue_size,
                                    producer_indices, self.sampler, self.num_workers)

            producer.start()
            self.producers.append(producer)






    def stop_threads(self):
        print('STOPPING THREADS ')
        data_stop_event.set()  # Signal threads to stop
        for producer in self.producers:
            if producer.is_alive():
                print(f"Joining producer")
                producer.join(timeout=5)  # Add a timeout to avoid hanging forever
                if producer.is_alive():
                    print(f"Producer did not exit, forcefully terminating...")
                    producer._stop()  # Force stop if the producer doesn't exit cleanly (not recommended, but an option)


        # Clean up the multiprocessing queue if using multiprocessing.Queue
        self.queue.close()
        self.queue.join_thread()
        print("Queue closed and joined.")

    def __iter__(self):

        self.batches_processed = 0  # Reset batch counter for the new epoch
        data_resume_event.set()  # Pause the producer
        data_stop_event.clear()

        print("Starting new epoch for rank:", self.rank, "queue size", self.queue.qsize)

        return self
    # def __next__(self):
    #     print("Batch processed:", self.batches_processed, "for rank:", self.rank, "epoch batches:", self.epoch_batches)
        
    #     if self.batches_processed >= self.epoch_batches -1 :
    #         print('RAISE StopIteration')

    
    #         raise StopIteration  # End of epoch


    #     while True:
    #         try:
    #             if 
    #             # Print the queue size specific to this GPU rank
    #             print(f"Queue size for GPU {self.rank}: {self.queue.qsize()} samples")

    #             # Get a batch from the queue
    #             batch = self.queue.get(timeout=1)
    #             self.batches_processed += 1

    #             # Print the processed batch info
    #             print(f"[GPU Rank {self.rank}] Processed batch {self.batches_processed}.")
                
    #             return batch

    #         except Empty:
    #             # If the queue is empty and the stop event is set, break the loop
    #             if data_stop_event.is_set():
    #                 print("Stopping iteration due to stop event.")
    #                 break
    # from queue import Empty

    def __next__(self):
        print("Batch processed:", self.batches_processed, "for rank:", self.rank, "epoch batches:", self.epoch_batches)

        if self.batches_processed >= self.epoch_batches:
            print('RAISE StopIteration')
            raise StopIteration  # End of epoch

        batch = []

        while len(batch) < self.batch_size:
            try:
                sample = self.queue.get(timeout=1)
                print(f"[GPU Rank {self.rank}] Fetched fast sample, current batch size: {len(batch)+1}")
            except Empty:
                try:
                    sample = self.slow_processed_queue.get(timeout=1)
                    print(f"[GPU Rank {self.rank}] Fetched slow sample, current batch size: {len(batch)+1}")
                except Empty:
                    if data_stop_event.is_set():
                        print("Stopping iteration due to stop event and empty queues.")
                        if not batch:
                            raise StopIteration
                        else:
                            break
                    else:
                        print(f"[GPU Rank {self.rank}] Waiting for samples...")
                        continue  # Try again

            if sample is not None:
                batch.append(sample)

        # if not batch:
        #     raise StopIteration

        images, labels = zip(*batch)
        images = [torch.from_numpy(img) for img in images]
        labels = [torch.from_numpy(lbl) if isinstance(lbl, np.ndarray) else lbl for lbl in labels]

        batch_images_tensor = torch.stack(images)
        batch_labels_tensor = torch.stack(labels)

        if batch_images_tensor.size(0) == self.batch_size:
            self.batches_processed += 1
            print(f"[GPU Rank {self.rank}] Completed batch {self.batches_processed}. Batch size: {batch_images_tensor.size(0)}")
            return batch_images_tensor, batch_labels_tensor
        else:
            print(f"[GPU Rank {self.rank}] Incomplete batch (size {batch_images_tensor.size(0)}), skipping.")
            return self.__next__()  # Retry until a full batch
